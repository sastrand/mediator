---
layout: post
title:  "SEDA: An Architecture for Well-Conditioned, Scalable Internet Services"
date:   2018-12-26 12:00:00
categories: paper_response
tags: 
image: /assets/article_images/20181226_seda/pipes.jpg
---

Every concurrent system is not either just multi-threaded or just an event-loop. SEDA shows that if the system itself can tolerate more programming complexity, the applications that run on it can benefit from the best parts of both architectures.

This is part of a series of responses I'm writing on classical papers in concurrency.

#### [Original paper](http://www.sosp.org/2001/papers/welsh.pdf) by Matt Welsh, David Culler, and Eric Brewer
#### Published in *Proceedings of the Eighteenth Symposium on Operating Systems Principles* (SOSP-18) <br> Banff, Canada. October 2001.

## High-Level Overview

  * When this paper was written in 2001, high-concurrency web servers were either purely multi-threaded or built with an event-loop maintained by a bounded thread pool of generally one thread per CPU.

  * They start off the paper observing that an unbounded thread pool with an arbitrary number of users will eventually lead to a system collapse in which the overhead of the TLB misses, scheduling, and lock contention among these threads consumes all the processor resources available for the threads to run. From this, they dismiss this design entirely for a web server.

  * By 2001, the basic event loop model was already the clear alternative to this problem. In the traditional event loop model, the application makes use of one thread per CPU: a manager thread and a fixed quantity of worker threads. The work of the web server will be divided into mutually exclusive, non-blocking tasks (they describe them in terms of individual finite state machines). 
    * When a request comes in, the manager thread sends it to the FSM corresponding to the work requested. When that work includes I/O, the thread handling that work makes the call to the I/O device and passes off the request to the manager thread, which queues it waiting for the I/O to complete, takes another request and passes it off to the waiting worker thread.
    * Described in these terms, it is a beautiful system. Throughput will flat-line relative to processing capacity and latency per user will increase linearly in proportion to the number of users waiting for a worker thread.

  In practice, however, the traditional event loop has some limitations as a model for web servers.

  * There's a performance limitation the assumption that all I/O can take place without blocking. While support for asynchronous I/O in the Linux kernel has grown considerably since the situation described by the authors in 2001, it's still [not perfect](https://lwn.net/Articles/724198/). This means some of threads on our very tightly constrained thread pool may actually block on I/O. This could lead, preposterously, to a CPU sitting idle with work available, but no thread to run the work. We'd like some extra to tap into in this case. 
  * The more significant limitation to this architecture is one of design. In order to achieve that awesome flat-line throughput relative to users, we're assuming each user has about the same amount of work they want done. If one of these FSM task loops is considerably longer than the others, an administrator of the system may want a policy to queue requests to that FSM before every thread is used so other, faster requests can still be served. We could add logic to the event-loop scheduler to do this, but that would require a modification to the scheduler every time we added CPUs and/or modified the FSMs to take more or less time relative to one another. This would lead to a significant loss in code modularity. 

The design suggested by Welsh et al. expands the traditional event-loop architecture by building out each FSM routine into a *stage* where a stage contains the same application logic as the FSM with the addition of its own event queue, its own thread pool or access to a thread pool shared among a few stages, and an event handler to manage its queue and thread resources. 

In effect, they observe the real-world limitations in the traditional event-loop architecture that could be solved with a more complex scheduler, but instead of adding layers of policy to the scheduler that would require modifications alongside modifications to routines in the server, they modularize the entire scheduler. This is the crux of SEDA.

* A more through description of a stage:
  * Each stage has an incoming event queue, an event-handler to manage this queue, and a thread pool. [awesome depiction page 5]
  * Each stage gets a small number of threads.
  * The event queue for each stage is finite—-when it fills up you can specify the behavior: degraded service, error message to the user, etc. Allowing high-traffic policies to be modularized at this level is already a significant benefit over the alternative architectures discussed in the paper.
  * A thread may only operate within a stage—and may only pass data across the control boundary by en-queuing an event. Doing this leads to a loss of locality in the cache, which may increase latency, but that latency is a constant factor relative to the number of requests.
* Resource controllers observe runtime data of the stage they’re in and make adjustments to meet performance goals.
  * The thread pool controller adjusts the number of threads executing within the stage. In the event of blocking I/O additional threads may be necessary, but too many threads slows performance. This controller can throttle this value.
  * The batching controller adjusts the number of events processed by each iteration of the event handler. Processing many events at once (batching) increases throughput but can increase response time. This trade-off may be worth it for some tasks, and again, the developer of the system can implement this trade-off in a modular way.
  * Both of these managers were built and tested to run with user threads.

## Performance Summary

The authors tested two implementations of the SEDA architecture against Apache, which at the time used a traditional event-loop design. They found that under heavy traffic loads, SEDA's throughput flat-lined at a higher level with more users. The graphs are pretty nice. Their data shows it can work well.

## Relationship to today

The last release of a true SEDA web server I could find is from an OSS project [JCyclone](http://jcyclone.sourceforge.net/) in 2005. A modern web server could be implemented to use the SEDA architecture using out-of-the-box message-passing components from [Apache ServiceMix](http://servicemix.apache.org/), but then you're building a distributed message passing system, which is its own design choice entirely.

If this lack of evidence for SEDA in the modern day is indeed evidence for absence, I would hypothesize that's not because of something wrong with SEDA but because of how much better user threads have gotten and how much cheaper it has become to build a fully distributed system. We've got microservices now where in SEDA we would have stages. We've got the flexibility of SEDA without its necessary added complexity at a cost of more hardware.

At a higher level, the dynamically managed, batch processing in AWS Kinesis and GCP Dataflow is not unlike the model proposed in SEDA. We want to divide the work we have to do into a bunch of discrete jobs. We want each of those discrete jobs to be able to use as many threads as possible and to dynamically manage their thread count. And we want work to queue up between these stages. In this way, SEDA absolutely lives on, just not as a web server.

If hardware becomes a limiting variables again in the design of concurrent systems, then something like SEDA will shine. For now, I appreciate it as evidence that every concurrent system is not either just multi-threaded or just an event-loop. There's room for all kinds of solutions in between.
