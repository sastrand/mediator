---
layout: post
title:  "SEDA: An Architecture for Well-Conditioned, Scalable Internet Services"
date:   2018-12-26 12:00:00
categories: paper_response
tags: 
image: /assets/article_images/20181226_seda/pipes.jpg
---

With custom synchonization primitives, you can use threads and event loops in a single service; the theoretical foundation of AWS Kinesis and GCP Dataflow.

This is part of a series of responses I'm writing on classical papers in concurrency.

#### [Original paper](http://www.sosp.org/2001/papers/welsh.pdf) by Matt Welsh, David Culler, and Eric Brewer
#### Published in *Proceedings of the Eighteenth Symposium on Operating Systems Principles* (SOSP-18) <br> Banff, Canada. October 2001.

## High-Level Overview

  * This paper proposes an alternative for highly concurrent work (like that performed by a webserver) that makes use of both threading and event-based concurrency models. 

  * Using many more threads than a computer has processors seems to lead almost unavoidably to performance degradation. The event-based model of concurrent operation allows for better performance with heavy traffic, but suffers from limited portability and modularity. The authors propose a structure for concurrent software development that solves the problems of event-based systems. The paper describes this “Staged, Event-driven Architecture” and the performance of one such implementation compared to two commonly used alternatives. 

  * SEDA includes elements of a threaded concurrency design, an event-driven server design, and a pipeline with each stage of the pipeline representing one stage in the overall architecture. It corresponds to the Birrell introductory thread paper we read for classes 2-3 by making good use of threading while carefully engineering against the logical and performance problems that can arise from imperfect thread implementations. 

  * The paper begins with a substantial description of SEDA followed by two trial applications of the architecture. These trial applications use the Jain fairness index (0-1, very similar to the Gini coefficient?) and what sounds like a reasonable simulation of real-world web traffic over a client-server and distributed type network.

  * The validation was convincing to me in within the confines of the paper itself, but as it was proposed by the authors as a new solution to an existing problem in 2001, I’m curious how it has been adopted and how it has performed in the real world.

  * The overall message of the paper is that highly concurrent programs can perform better when built on top of architectures (like SEDA) that make careful, extensible use of synchronization primitives as compared to program implementations that bypass such an architecture and employ these primitives unaided. 

## Review of Technical Aspects

* Introduction: as the demand for Internet services grows, new system design techniques must be use to manage this load.
  * Three trends make the problem of high-traffic webserver management more challenging:
    * Servers themselves are becoming more complex, requiring more IO.
    * Service logic changes rapidly.
    * Services are increasingly hosted on general-purpose hardware.
  * The crux: good synchronization primitives allow for robust performance on a wide range of services subject to huge variations in load, while preserving ease of authorship (whew).
  * To achieve this, SEDA combines aspects of threads and event-based programming models to manage: concurrency, IO, scheduling, and resource management needs of Internet services (is there anything else an Internet service needs?) 

* There are several alternative implementation of a concurrent web-server within the threads and event-based system, but each one has drawbacks that can be overcome with SEDA.
  * Unbound, thread-based concurrency requires such high overhead to manage each new thread generated by each new service request that as load increases, throughput and response-time decrease at a beyond linear rate. 
    * These overheads are caused by: TLB misses, scheduling, and lock contention. 
    * Threads work best at a low level to divide work among processors.
  * Bounded thread pools avoid throughput degradation at the cost of great unfairness. 
  * Event-driven concurrency appears to be an implementation of everything as a producer-consumer problem. 
    * A server has typically one thread per processor that loop, processing events of differnet types from a queue.
    * With IO done separate from work on the event queue, each task can be implemented as an FSM, where transitions between states in the FSM are triggered by events. [p 3] Awesome, but how does this work?
    * In theory this is the dream of a high concurrency program, but to implement ideally would require non-blocking primitives (that don’t exist).
    * Also careful programming is required to multiplex the processing of multiple FSMs.
    * And modularity is difficult to achieve as the code built on top of this architecture must not itself block or consume a lot of resources.
  * Structured event queues perform well by grouping similarly typed work together, increasing locality, and ensuring fairness by bounding module processing times. By using a whole set of event queues, modularity and design complexity improve. The authors of this paper believe there are difficulties that remain, inherent to adapting a thread-based model to high loads without explicit request queues (and adding these explicit request queues and some automatic primitive management is SEDA)

* SEDA decomposes an application into a network of stages separated by event queues and introduces the notion of dynamic resource controllers to allow applications to adjust dynamically to changing load.
  * The fundamental unit of processing in SEDA is a stage.
    * Each stage has an event-handler (the work to be done), an incoming event queue, and a thread pool. [awesome depiction page 5]
    * Each stage gets a small number of threads—these are the “basic concurrency”
    * Event queues can be finite—when they fill up you can specify the behavior, but to avoid memory leaks and hold down performance variance, the authors seem to suggest using the threshold feature of event queues.
    * A thread may only operate within a stage—and may only pass data across the control boundary by en-queueing an event. This may increase latency, but is the heart of the architecture’s modularity, which is useful for debugging and performance analysis—processes otherwise more difficult in multi-threaded servers.
  * Resource controllers observe runtime data of the stage they’re in and make adjustments to meet performance goals.
    * The thread pool controller adjusts the number of threads executing within the stage. Too many threads slows performance.
    * The batching controller adjusts the number of events processed by each iteration of the event handler. Processing many events at once (batching) increases throughput but can increase response time. 
    * These managers don’t concern themselves with the underlying thread implementation. The writers believe any good scheduler is adequate for the needs of an Internet service SEDA application.
  * Sandstorm is a SEDA prototype. It provides APIs for naming, creating, and destroying stages, performing queue operations, controlling queue thresholds, and profiling and debugging. It does not let you directly interact with threads, though. It’s written in Java.
* A SEDA implementation makes use of a systems existing asynchronous thread primitives. In this section, an implementation for a webserver is discussed.
  * Three stages are used to maintain a network connection: one to read, write, and listen. 
  * The readStage has an optional throttle on the rate at which packets are read from the network. 
  * `writeStage` may be thresholded to prevent slow sockets from consuming to many resources. 
  * Simply by avoiding blocking, this three stage approach outperforms a threaded model on any sizable load.
  * In Sandstorm, blocking primitives must be used as they are the only option. {Why is this? What are non-blocking primatives?} To avoid blocking, though, only one thread may process events for a single file at a time.

* Haboob is a web server consisting of 10 stages tested against Flash and Apache. 
  * The Jain fairness index is:
    * (the number of requests from all the clients in a network)squared 
divided by
    * the number of clients times(the sum of the number of requests from each  client squared). [p 9]
    * This is cool, but intuitively, how does it work?
  * Although all three servers have approximately the same average response time, the distribution is very different. Apache and Flash have a high standard deviation compared to Haboob. This long tail is caused by “exponential back-off in the TCP retransmission timer for establishing a new connection” What is this?
  * Haboob can adaptively shed costly loads. Flash sheds loads due to a bug, but this was inspiring to the Haboob developers as a way to bound the response time of requests in the server. Once the average response time drops below 5 seconds, the controller exponentially reduces the stage’s queue threshold. 

* Gnutella is a peer-to-peer network used to test traffic in a non-client-server model. The writers observed bottlenecks with attempts to write to ultimately slow network connections. To solve this, they implemented another time-out policy that just drops a packet when a time-out is reached.

* In conclusion, the writers “believe that measurement and control is the key to resource management and overload protection in busy Internet services. This is in contrast to long-standing approaches based on resource containment, which assign fixed resources to each task.”
